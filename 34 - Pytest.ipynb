{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit tests - Pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unit test is a type of software testing where individual components or units of a software system are tested in isolation. The goal of unit testing is to verify that each unit of the software performs as designed. In the context of object-oriented programming, a \"unit\" typically refers to an individual method or function.\n",
    "\n",
    "Pytest is a testing framework for Python that makes it easy to write simple unit tests as well as complex functional tests. It is widely used in the Python ecosystem due to its simplicity, powerful features, and extensive plugin support. Pytest supports test discovery, fixtures, parameterized testing, and various types of assertions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key characteristics of unit tests include**\n",
    "\n",
    "<br>\n",
    "\n",
    "**Isolation:** Unit tests are designed to be isolated from the rest of the system. They should only test the behavior of a specific unit (e.g., a function, method, or class) without relying on or being affected by other components.\n",
    "\n",
    "**Automation:** Unit tests are automated, meaning they are executed by testing frameworks or tools rather than manually. This allows for frequent and consistent testing during development and helps catch regressions.\n",
    "\n",
    "**Fast Execution:** Unit tests are expected to execute quickly. Fast execution is essential to facilitate running tests frequently, such as during development or as part of a continuous integration process.\n",
    "\n",
    "**Repeatable:** Unit tests should produce the same results each time they are executed. This ensures consistency and reproducibility, making it easier to identify and fix issues.\n",
    "\n",
    "**Independent:** Unit tests should be independent of each other. The success or failure of one test should not impact the execution of another. This independence allows for better identification of the root cause when a test fails.\n",
    "\n",
    "**Focused Scope:** Unit tests focus on a specific functionality or behavior of a unit. They should not test the entire application or involve external dependencies, such as databases or network services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to setup files/functions for Pytest?\n",
    "\n",
    "First we have to install/import pytest.\n",
    "\n",
    "The .py file, which we want to test should be named as either test_*.py or *_test.py. The name of the functions (which are to be tested) in that script should start with test*. \n",
    "\n",
    "In the specified functions we need to use the assert keyword to test our assumptions. The below examples show one pytest, which always passes and one, which always fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_always_passes():\n",
    "    assert True\n",
    "\n",
    "def test_always_fails():   \n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running pytest without mentioning a filename will run all files of format test_*.py or *_test.py in the current directory and subdirectories. Pytest automatically identifies those files as test files. We can make pytest run other filenames by explicitly mentioning them.\n",
    "\n",
    "Pytest requires the test function names to start with test. Function names which are not of format test* are not considered as test functions by pytest. We cannot explicitly make pytest consider any function not starting with test as a test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_square.py\n",
    "\n",
    "import math\n",
    "\n",
    "def test_sqrt():\n",
    "   num = 25\n",
    "   assert math.sqrt(num) == 5\n",
    "\n",
    "def testsquare():\n",
    "   num = 7\n",
    "   assert 7*7 == 40\n",
    "\n",
    "def tesequality():\n",
    "   assert 10 == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run pytest by writing pytest in the terminal.\n",
    "\n",
    "Running pytest on test_square.py will only investigate the first 2 functions, because pytest will not consider 'tesequality' as a test since its name is not of the format test*.\n",
    "\n",
    "If run the 'pytest' command in the terminal then it will show as a green dot and an F. That means, that the first test function passed the test and the second testfunction failed.\n",
    "\n",
    "If we write 'pytest -v' in the terminal, then it will explicitly show the function names, which passed or failed.\n",
    "\n",
    "In case we want to run pytest for a specific file (e.g., test_compare.py) then we can use the following syntax in the terminal:\n",
    "\n",
    "pytest test_compare.py -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_compare.py\n",
    "\n",
    "def test_greater():\n",
    "   num = 100\n",
    "   assert num > 100\n",
    "\n",
    "def test_greater_equal():\n",
    "   num = 100\n",
    "   assert num >= 100\n",
    "\n",
    "def test_less():\n",
    "   num = 100\n",
    "   assert num < 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute the tests containing a string in its name we can use the following syntax:\n",
    "\n",
    "pytest -k great -v\n",
    "\n",
    "This command will execute all the test names having the word ‘great’ in its name. In this case, they are test_greater() and test_greater_equal(). The name of the test function should still start with ‘test’.\n",
    "\n",
    "Pytest allows us to use markers on test functions. Markers are used to set various features/attributes to test functions. Pytest provides many inbuilt markers such as xfail, skip and parametrize. Apart from that, users can create their own marker names. Markers are applied on the tests using the syntax given below:\n",
    "\n",
    "@pytest.mark.markername\n",
    "\n",
    "To use markers, we have to import pytest module in the test file. We can define our own marker names to the tests and run the tests having those marker names.\n",
    "\n",
    "To run the marked tests, we can use the following syntax:\n",
    "\n",
    "pytest -m markername -v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_square2.py\n",
    "\n",
    "import pytest\n",
    "import math\n",
    "\n",
    "@pytest.mark.square\n",
    "def test_sqrt():\n",
    "   num = 25\n",
    "   assert math.sqrt(num) == 5\n",
    "\n",
    "@pytest.mark.square\n",
    "def testsquare():\n",
    "   num = 7\n",
    "   assert 7*7 == 40\n",
    "\n",
    "@pytest.mark.others\n",
    "def test_equality():\n",
    "   assert 10 == 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_compare2.py\n",
    "\n",
    "import pytest\n",
    "@pytest.mark.great\n",
    "def test_greater():\n",
    "   num = 100\n",
    "   assert num > 100\n",
    "\n",
    "@pytest.mark.great\n",
    "def test_greater_equal():\n",
    "   num = 100\n",
    "   assert num >= 100\n",
    "\n",
    "@pytest.mark.others\n",
    "def test_less():\n",
    "   num = 100\n",
    "   assert num < 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the following command:\n",
    "\n",
    "pytest -m others -v\n",
    "\n",
    "then it will run test_equality and test_less functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_div_by_3_6\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def input_value():\n",
    "   input = 39\n",
    "   return input\n",
    "\n",
    "def test_divisible_by_3(input_value):\n",
    "   assert input_value % 3 == 0\n",
    "\n",
    "def test_divisible_by_6(input_value):\n",
    "   assert input_value % 6 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Fixtures\n",
    "    \n",
    "Fixtures are functions, which will run before each test function to which it is applied. Fixtures are used to feed some data to the tests such as database connections, URLs to test and some sort of input data. Therefore, instead of running the same code for every test, we can attach fixture function to the tests and it will run and return the data to the test before executing each test.\n",
    "\n",
    "A function is marked as a fixture by:\n",
    "\n",
    "@pytest.fixture\n",
    "\n",
    "Here, we have a fixture function named input_value, which supplies the input to the tests. To access the fixture function, the tests have to mention the fixture name as input parameter.\n",
    "\n",
    "Pytest while the test is getting executed, will see the fixture name as input parameter. It then executes the fixture function and the returned value is stored to the input parameter, which can be used by the test.\n",
    "\n",
    "A fixture function defined inside a test file has a scope within the test file only. We cannot use that fixture in another test file. To make a fixture available to multiple test files, we have to define the fixture function in a file called conftest.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    conftest.py\n",
    "\n",
    "We can define the fixture functions in this file to make them accessible across multiple test files. For that reason we have to create a file named conftest.py and create a fixture there.\n",
    "\n",
    "If I run the pytest on test_div_by_13.py, then it will recognize the pytest fixture, even though it doesn't contain it.\n",
    "\n",
    "pytest test_div_by_13.py -v\n",
    "\n",
    "The tests will look for fixture in the same file. As the fixture is not found in the file, it will check for fixture in conftest.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Parameterizing tests\n",
    "\n",
    "Parameterizing of a test is done to run the test against multiple sets of inputs. We can do this by using the following marker:\n",
    "\n",
    "@pytest.mark.parametrize\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_multiplication.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"num, output\",[(1,11),(2,22),(3,35),(4,44)])\n",
    "def test_multiplication_11(num, output):\n",
    "   assert 11*num == output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the test multiplies an input with 11 and compares the result with the expected output. The test has 4 sets of inputs, each has 2 values – one is the number to be multiplied with 11 and the other is the expected result.\n",
    "\n",
    "Pytest -k multiplication -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Xfail/Skip tests\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, consider the below situations:\n",
    "\n",
    "A test is not relevant for some time due to some reasons.\n",
    "\n",
    "A new feature is being implemented and we already added a test for that feature.\n",
    "\n",
    "<br>\n",
    "\n",
    "In these situations, we have the option to xfail the test or skip the tests.\n",
    "\n",
    "Pytest will execute the xfailed test, but it will not be considered as part failed or passed tests. Details of these tests will not be printed even if the test fails (remember pytest usually prints the failed test details). We can xfail tests using the following marker:\n",
    "\n",
    "@pytest.mark.xfail\n",
    "\n",
    "Skipping a test means that the test will not be executed. We can skip tests using the following marker:\n",
    "\n",
    "@pytest.mark.skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_compare3\n",
    "\n",
    "import pytest\n",
    "@pytest.mark.xfail\n",
    "@pytest.mark.great\n",
    "def test_greater():\n",
    "   num = 100\n",
    "   assert num > 100\n",
    "\n",
    "@pytest.mark.xfail\n",
    "@pytest.mark.great\n",
    "def test_greater_equal():\n",
    "   num = 100\n",
    "   assert num >= 100\n",
    "\n",
    "@pytest.mark.skip\n",
    "@pytest.mark.others\n",
    "def test_less():\n",
    "   num = 100\n",
    "   assert num < 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I run the following command:\n",
    "\n",
    "pytest test_compare3.py -v\n",
    "\n",
    "It will show me that the first function got xfailed, the second got xpassed and the third got skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define that the pytest should stop after a certain number of failed tests. We can do that with the following syntax:\n",
    "\n",
    "pytest file_name --maxfail = number"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
